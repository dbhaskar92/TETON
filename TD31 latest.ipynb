{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxAVc8hsvHLH"
      },
      "outputs": [],
      "source": [
        "pip install numpy scipy pandas scikit-learn cvxpy group-lasso pysindy\n",
        "!pip install --upgrade importlib-metadata\n",
        "!pip install --upgrade derivative\n",
        "!pip install --upgrade --force-reinstall pysindy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # Import the drive module from google.colab\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)  # Remount with force_remount=True\n",
        "!find /content/drive/MyDrive/data\n"
      ],
      "metadata": {
        "id": "rBXFvRvPvskn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -----------------------------------------------------------\n",
        "# Full Pipeline (per-window closed complexes + adaptive tau):\n",
        "# 1. Data Loading & Per-channel Normalization\n",
        "# 2. Robust Locality Pre-selection (PC-based MAD metrics)\n",
        "# 3. Dynamic SINDy Fitting on Selected Local Points (Taylor Centering)\n",
        "# 4. Per-window simplicial complex (unweighted) with closure + counts\n",
        "# 5. Optional global structural/topology summary (unions across windows)\n",
        "# 6. Save per-window counts + summary JSON\n",
        "# -----------------------------------------------------------\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import savgol_filter\n",
        "import pysindy as ps\n",
        "from joblib import Parallel, delayed\n",
        "from collections import defaultdict\n",
        "import heapq\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ USER PARAMETERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "CSV_FILE        = '/content/drive/MyDrive/data/eeg/EEG_TD_31_EEGdata.csv'\n",
        "FS              = 256.0          # [Hz]\n",
        "WIN_SG, ORDER   = 13, 3          # SavGol window (odd) & polynomial order\n",
        "D_MAX           = 2              # 2‚Üí up to triangles; 3‚Üí allow tetrahedra\n",
        "THRESH_SINDY    = 0.2            # STLSQ sparsity threshold (model-fitting phase)\n",
        "\n",
        "# Adaptive mapping threshold (per window) for |coef| ‚Üí simplex\n",
        "TAU_MODE        = 'percentile'   # 'percentile' or 'mad'\n",
        "TAU_PCTL        = 97.5           # if TAU_MODE='percentile'\n",
        "TAU_KMAD        = 6.0            # if TAU_MODE='mad' ‚Üí tau = median + KMAD * MAD\n",
        "ENFORCE_CLOSURE = True           # make each window's output a simplicial complex\n",
        "\n",
        "WIN_LEN         = 1024           # samples per sliding window\n",
        "STRIDE          = 512            # hop between windows\n",
        "MAX_ROWS        = 3000           # cap rows per window during SINDy\n",
        "N_JOBS          = -1             # joblib cores (-1 = all)\n",
        "\n",
        "# Locality Analysis Parameters (PC-based)\n",
        "R_TARGET_PC  = 1.0     # ~1 z-unit RMS per channel kept\n",
        "R_DROP95_PC  = 3.5     # drop only if window is wildly nonlocal\n",
        "BAD_KEPT_PC  = 2.0     # if kept subset is still too wide, skip\n",
        "K_MIN        = 600     # minimum kept timestamps\n",
        "K_MAX        = 1000    # optional cap\n",
        "\n",
        "# Analysis Options\n",
        "CENTER_METHOD   = 'median'       # 'mean' or 'median' for Taylor expansion center\n",
        "\n",
        "OUTDIR          = 'dyn_graphs_full_pipeline_pc_norm_counts'\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "# 1 ‚îÄ‚îÄ‚îÄ READ (T √ó n ‚Üí n √ó T)\n",
        "raw = pd.read_csv(\n",
        "        CSV_FILE,\n",
        "        header=None,\n",
        "        nrows=None,\n",
        "        usecols=range(31)      # 31 channels\n",
        "      ).values.astype(np.float64)\n",
        "\n",
        "# Drop constant/all-zero channels\n",
        "tol = 1e-12\n",
        "nz_mask = (np.ptp(raw, axis=0) > tol)\n",
        "if (~nz_mask).any():\n",
        "    dropped = np.where(~nz_mask)[0] + 1\n",
        "    print(f'‚ö†Ô∏è  Dropping constant channels (1-based): {dropped.tolist()}')\n",
        "raw = raw[:, nz_mask]\n",
        "raw = raw.T                       # rows = channels, cols = time\n",
        "n, T_raw = raw.shape\n",
        "dt = 1 / FS\n",
        "\n",
        "print(f\"Loaded data: {n} channels √ó {T_raw} samples ({T_raw/FS:.1f} seconds)\")\n",
        "\n",
        "# 2 ‚îÄ‚îÄ‚îÄ SAVITZKY‚ÄìGOLAY smooth + derivative (aligned, same length)\n",
        "half = (WIN_SG - 1) // 2\n",
        "X_smooth_full = savgol_filter(raw, WIN_SG, ORDER, axis=1, mode='interp')\n",
        "dXdt_full     = savgol_filter(raw, WIN_SG, ORDER, deriv=1, delta=dt, axis=1, mode='interp')\n",
        "\n",
        "# Trim ends equally to reduce edge artifacts and keep X and dXdt aligned\n",
        "X_smooth = X_smooth_full[:, half:-half]\n",
        "dXdt_raw = dXdt_full[:,       half:-half]\n",
        "\n",
        "# 3 ‚îÄ‚îÄ‚îÄ Per-channel Normalization (X to z-score; Y scaled by same sig)\n",
        "print(\"Applying per-channel normalization...\")\n",
        "eps = 1e-8\n",
        "mu  = X_smooth.mean(axis=1, keepdims=True)\n",
        "sig = X_smooth.std(axis=1, keepdims=True) + eps\n",
        "\n",
        "X = (X_smooth - mu) / sig\n",
        "Y = dXdt_raw / sig  # scale only\n",
        "\n",
        "norm_params = {\"mean\": mu.squeeze().tolist(), \"std\": sig.squeeze().tolist()}\n",
        "print(\"Normalization complete.\")\n",
        "print(f\"Data after preprocessing: {X.shape[0]} channels √ó {X.shape[1]} samples\")\n",
        "print(\"-\" * 34)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Locality Analysis Helpers (PC-based) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def robust_window_distances(Xw_centered, eps=1e-8):\n",
        "    \"\"\"Per-channel robust z-distance for each timestamp in a window.\"\"\"\n",
        "    med   = np.median(Xw_centered, axis=1, keepdims=True)\n",
        "    mad   = np.median(np.abs(Xw_centered - med), axis=1, keepdims=True)\n",
        "    sigma = np.maximum(eps, 1.4826 * mad)\n",
        "    Z = Xw_centered / sigma\n",
        "    d_raw = np.sqrt((Z**2).sum(axis=0))     # L2 across channels\n",
        "    d_pc  = d_raw / np.sqrt(Z.shape[0])     # per-channel RMS z-distance\n",
        "    return d_pc, sigma\n",
        "\n",
        "def select_local_indices(Xw):\n",
        "    \"\"\"Pick local timestamps based on robust per-channel distances.\"\"\"\n",
        "    x0 = np.median(Xw, axis=1, keepdims=True)\n",
        "    Xc = Xw - x0\n",
        "    d_pc, sigma = robust_window_distances(Xc)\n",
        "    p95_pc = float(np.percentile(d_pc, 95))\n",
        "\n",
        "    keep = np.where(d_pc <= R_TARGET_PC)[0]\n",
        "    used = \"radius_pc\"\n",
        "    if keep.size < K_MIN:\n",
        "        keep = np.argsort(d_pc)[:K_MIN]; used = \"kmin_pc\"\n",
        "    elif keep.size > K_MAX:\n",
        "        keep = np.argsort(d_pc)[:K_MAX]; used = \"kmax_pc\"\n",
        "\n",
        "    p95_kept_pc = float(np.percentile(d_pc[keep], 95)) if keep.size > 0 else np.nan\n",
        "\n",
        "    if (p95_pc > R_DROP95_PC) and (p95_kept_pc > BAD_KEPT_PC):\n",
        "        return {\"skip\": True, \"x0\": x0, \"sigma\": sigma,\n",
        "                \"p95_raw\": p95_pc, \"p95_kept\": p95_kept_pc, \"used\": \"drop_pc\"}\n",
        "\n",
        "    return {\"skip\": False, \"x0\": x0, \"sigma\": sigma, \"keep\": np.sort(keep),\n",
        "            \"p95_raw\": p95_pc, \"p95_kept\": p95_kept_pc, \"used\": used}\n",
        "\n",
        "def preselect_windows(X, WIN_LEN, STRIDE):\n",
        "    metas = []\n",
        "    starts = range(0, X.shape[1] - WIN_LEN + 1, STRIDE)\n",
        "    nW = len(starts)\n",
        "    print(\"\\n--- Performing Robust Locality Pre-selection ---\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Analyzing {nW} windows with WIN_LEN={WIN_LEN}, STRIDE={STRIDE}...\")\n",
        "    print(f\"Locality Params: R_TARGET_PC={R_TARGET_PC}, K_MIN={K_MIN}, K_MAX={K_MAX}, \"\n",
        "          f\"R_DROP95_PC={R_DROP95_PC}, BAD_KEPT_PC={BAD_KEPT_PC}\\n\")\n",
        "\n",
        "    for wi, w_start in enumerate(starts, 1):\n",
        "        w_end = w_start + WIN_LEN\n",
        "        Xw = X[:, w_start:w_end]\n",
        "        out = select_local_indices(Xw)\n",
        "        t_mid = (w_start + w_end) / 2 * dt\n",
        "\n",
        "        if out[\"skip\"]:\n",
        "            print(f\"Window {wi}/{nW} (t={t_mid:.3f}s): SKIP ({out['used']}) \"\n",
        "                  f\"p95_raw={out['p95_raw']:.2f}, p95_kept={out['p95_kept']:.2f}\")\n",
        "            metas.append({\n",
        "                \"w_start\": w_start, \"t_mid\": t_mid, \"skip\": True, \"keep\": [],\n",
        "                \"p95_raw\": out[\"p95_raw\"], \"p95_kept\": out[\"p95_kept\"],\n",
        "                \"kept_pct\": 0.0, \"used\": out[\"used\"]\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        keep = out[\"keep\"]\n",
        "        kept_pct = 100.0 * keep.size / WIN_LEN\n",
        "        print(f\"Window {wi}/{nW} (t={t_mid:.3f}s): kept={keep.size}/{WIN_LEN} ({kept_pct:.1f}%), \"\n",
        "              f\"p95_raw={out['p95_raw']:.2f}, p95_kept={out['p95_kept']:.2f}, mode={out['used']}\")\n",
        "\n",
        "        metas.append({\n",
        "            \"w_start\": w_start, \"t_mid\": t_mid, \"skip\": False, \"keep\": keep.tolist(),\n",
        "            \"x0\": out[\"x0\"].flatten().tolist(), \"sigma\": out[\"sigma\"].flatten().tolist(),\n",
        "            \"p95_raw\": out[\"p95_raw\"], \"p95_kept\": out[\"p95_kept\"],\n",
        "            \"kept_pct\": float(kept_pct), \"used\": out[\"used\"]\n",
        "        })\n",
        "\n",
        "    kept_counts = [len(m[\"keep\"]) for m in metas if not m[\"skip\"]]\n",
        "    drops = sum(m[\"skip\"] for m in metas)\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"--- Locality Pre-selection Summary ---\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Total Windows: {nW}   Dropped (nonlocal): {drops}\")\n",
        "    if kept_counts:\n",
        "        print(f\"Points kept per window (min/mean/median/max): \"\n",
        "              f\"{min(kept_counts)}/{np.mean(kept_counts):.1f}/{np.median(kept_counts):.1f}/{max(kept_counts)}\")\n",
        "\n",
        "    # Save per-window locality diagnostics\n",
        "    df = pd.DataFrame([{\n",
        "        \"w_start\": m[\"w_start\"], \"t_mid\": m[\"t_mid\"], \"skip\": m[\"skip\"],\n",
        "        \"kept\": len(m[\"keep\"]), \"kept_pct\": m[\"kept_pct\"],\n",
        "        \"p95_raw\": m[\"p95_raw\"], \"p95_kept\": m[\"p95_kept\"], \"mode\": m[\"used\"]\n",
        "    } for m in metas])\n",
        "    df.to_csv(f\"{OUTDIR}/locality_stats_pc_perchannelnorm.csv\", index=False)\n",
        "    print(f\"‚úì Saved locality stats to {OUTDIR}/locality_stats_pc_perchannelnorm.csv\")\n",
        "    return metas\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ SINDy + hyperedge extraction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def indices_from_term(term_str):\n",
        "    \"\"\"\n",
        "    Parse term string to a list of variable indices.\n",
        "      '1'          -> []\n",
        "      'x0'         -> [0]\n",
        "      'x0^2'       -> [0, 0]\n",
        "      'x1 x4'      -> [1, 4]\n",
        "      'x1^3 x4'    -> [1, 1, 1, 4]\n",
        "    \"\"\"\n",
        "    if term_str == '1':\n",
        "        return []\n",
        "    idxs = []\n",
        "    for tok in term_str.split():\n",
        "        base, *pow_part = tok.split('^')\n",
        "        j = int(base[1:])\n",
        "        power = int(pow_part[0]) if pow_part else 1\n",
        "        idxs.extend([j] * power)\n",
        "    return idxs\n",
        "\n",
        "def coefs_to_simplex_sets(coefs, feature_names, *, thresh, enforce_closure=True, return_direct=False):\n",
        "    \"\"\"\n",
        "    Build UNWEIGHTED simplices for ONE WINDOW from coefficients:\n",
        "      ‚Ä¢ Use a per-window threshold 'thresh' on |coef|\n",
        "      ‚Ä¢ Skip bias '1', repeated indices, and any term containing the target as a factor\n",
        "      ‚Ä¢ If enforce_closure=True, include all faces of each simplex\n",
        "    Returns closed sets; with return_direct=True also returns direct (pre-closure) sets.\n",
        "    \"\"\"\n",
        "    parsed = [None if t == '1' else indices_from_term(t) for t in feature_names]\n",
        "    n = coefs.shape[0]\n",
        "\n",
        "    edges_dir, tris_dir, quads_dir = set(), set(), set()\n",
        "\n",
        "    # Collect direct candidate simplices (unweighted)\n",
        "    for target in range(n):\n",
        "        row = coefs[target]\n",
        "        for w, idxs in zip(row, parsed):\n",
        "            if idxs is None or abs(w) < thresh:\n",
        "                continue\n",
        "            # genuine multi-node: no repeated indices\n",
        "            if len(set(idxs)) != len(idxs):\n",
        "                continue\n",
        "            all_nodes = [target] + idxs\n",
        "            # require all nodes distinct (prevents target participation)\n",
        "            if len(set(all_nodes)) != len(all_nodes):\n",
        "                continue\n",
        "            k = len(all_nodes)\n",
        "            s = frozenset(all_nodes)\n",
        "            if   k == 2: edges_dir.add(s)\n",
        "            elif k == 3: tris_dir.add(s)\n",
        "            elif k == 4: quads_dir.add(s)\n",
        "\n",
        "    # Enforce closure (faces)\n",
        "    edges_cl, tris_cl, quads_cl = set(edges_dir), set(tris_dir), set(quads_dir)\n",
        "    if enforce_closure:\n",
        "        for q in list(quads_cl):\n",
        "            for face3 in combinations(q, 3):\n",
        "                tris_cl.add(frozenset(face3))\n",
        "            for face2 in combinations(q, 2):\n",
        "                edges_cl.add(frozenset(face2))\n",
        "        for t in list(tris_cl):\n",
        "            for face2 in combinations(t, 2):\n",
        "                edges_cl.add(frozenset(face2))\n",
        "\n",
        "    if return_direct:\n",
        "        return edges_cl, tris_cl, quads_cl, edges_dir, tris_dir, quads_dir\n",
        "    return edges_cl, tris_cl, quads_cl\n",
        "\n",
        "# Global polynomial library (linear + interactions up to D_MAX)\n",
        "GLOBAL_LIBRARY = ps.PolynomialLibrary(\n",
        "    degree=D_MAX,\n",
        "    include_bias=False,\n",
        "    interaction_only=False\n",
        ")\n",
        "\n",
        "def _tau_from_abs(absA):\n",
        "    \"\"\"Compute per-window tau from |coef| matrix according to TAU_MODE.\"\"\"\n",
        "    v = absA.ravel()\n",
        "    if TAU_MODE.lower() == 'mad':\n",
        "        med = np.median(v)\n",
        "        mad = 1.4826 * np.median(np.abs(v - med))\n",
        "        return float(med + TAU_KMAD * mad)\n",
        "    # default: percentile\n",
        "    return float(np.percentile(v, TAU_PCTL))\n",
        "\n",
        "def fit_window_from_meta(meta):\n",
        "    if meta[\"skip\"]:\n",
        "        return {\"t_mid\": meta[\"t_mid\"], \"skip\": True, \"reason\": \"nonlocal window\"}\n",
        "\n",
        "    w_start = meta[\"w_start\"]; w_end = w_start + WIN_LEN\n",
        "    Xw_orig, Yw_orig = X[:, w_start:w_end], Y[:, w_start:w_end]\n",
        "\n",
        "    x0   = np.array(meta[\"x0\"]).reshape(-1, 1)\n",
        "    keep = np.array(meta[\"keep\"])\n",
        "    if keep.size == 0:\n",
        "        return {\"t_mid\": meta[\"t_mid\"], \"skip\": True, \"reason\": \"empty keep list\"}\n",
        "\n",
        "    Xw_c = Xw_orig - x0\n",
        "    Xw_use, Yw_use = Xw_c[:, keep], Yw_orig[:, keep]\n",
        "\n",
        "    if Xw_use.shape[1] > MAX_ROWS:\n",
        "        idx = np.linspace(0, Xw_use.shape[1]-1, MAX_ROWS, dtype=int)\n",
        "        Xw_use, Yw_use = Xw_use[:, idx], Yw_use[:, idx]\n",
        "\n",
        "    optimizer = ps.STLSQ(alpha=1e-3, threshold=THRESH_SINDY)\n",
        "    model = ps.SINDy(feature_library=GLOBAL_LIBRARY, optimizer=optimizer)\n",
        "    model.fit(Xw_use.T, t=dt, x_dot=Yw_use.T, quiet=True)\n",
        "\n",
        "    A = model.coefficients()\n",
        "    names = model.get_feature_names()\n",
        "\n",
        "    # NEW: extract raw scores (no threshold here)\n",
        "    S2, S3 = extract_edge_triangle_scores(A, names, n)\n",
        "\n",
        "    return {\n",
        "        't_mid': meta[\"t_mid\"],\n",
        "        'S2': S2,                         # (n x n) symmetric edge scores\n",
        "        'S3': S3,                         # dict of triangle scores\n",
        "        'x0': x0.flatten().tolist(),\n",
        "        'n_samples_fit': int(Xw_use.shape[1]),\n",
        "        'n_samples_kept': len(meta[\"keep\"]),\n",
        "        'kept_pct': meta[\"kept_pct\"],\n",
        "        'locality_mode': meta[\"used\"],\n",
        "        'p95_raw': meta[\"p95_raw\"],\n",
        "        'p95_kept': meta[\"p95_kept\"],\n",
        "        'skip': False\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def _parse_feature_types_via_indices(feature_names):\n",
        "    kinds, lin_var, cross_pair = [], [], []\n",
        "    for name in feature_names:\n",
        "        idxs = [] if name == '1' else indices_from_term(name)\n",
        "        if len(idxs) == 1:\n",
        "            kinds.append('lin');  lin_var.append(idxs[0]); cross_pair.append(None)\n",
        "        elif len(idxs) == 2 and len(set(idxs)) == 2:\n",
        "            i, j = sorted(idxs)\n",
        "            kinds.append('cross'); lin_var.append(None);    cross_pair.append((i, j))\n",
        "        else:\n",
        "            kinds.append('other'); lin_var.append(None);    cross_pair.append(None)\n",
        "    return kinds, lin_var, cross_pair\n",
        "\n",
        "def extract_edge_triangle_scores(A, feature_names, n):\n",
        "    \"\"\"\n",
        "    Return:\n",
        "      S2: (n x n) symmetric edge scores (only linear features)\n",
        "      S3: dict {frozenset({i,j,k}): score} of triangle scores\n",
        "    \"\"\"\n",
        "    kinds, lin_of, cross_pair = _parse_feature_types_via_indices(feature_names)\n",
        "    lin_cols   = [c for c,k in enumerate(kinds) if k == 'lin']\n",
        "    cross_cols = [c for c,k in enumerate(kinds) if k == 'cross']\n",
        "    absA = np.abs(A)\n",
        "\n",
        "    # edges: symmetric from linear terms only\n",
        "    S2_dir = np.zeros((n, n), dtype=float)\n",
        "    for i in range(n):                # target\n",
        "        for c in lin_cols:\n",
        "            j = lin_of[c]\n",
        "            if j is None or j == i:\n",
        "                continue\n",
        "            S2_dir[i, j] = max(S2_dir[i, j], absA[i, c])\n",
        "    S2 = np.maximum(S2_dir, S2_dir.T) # undirected\n",
        "\n",
        "    # triangles: genuine cross terms (target i, monomial x_j x_k with j<k, j!=i, k!=i)\n",
        "    S3 = defaultdict(float)\n",
        "    for i in range(n):   # target\n",
        "        for c in cross_cols:\n",
        "            j, k = cross_pair[c]\n",
        "            if i in (j, k):  # skip target participation in the monomial\n",
        "                continue\n",
        "            key = frozenset((i, j, k))\n",
        "            if absA[i, c] > S3[key]:\n",
        "                S3[key] = absA[i, c]\n",
        "    return S2, S3\n",
        "\n",
        "# --- maximum bottleneck spanning tree bottleneck value (per window) ---\n",
        "def mst_bottleneck_value(S2):\n",
        "    \"\"\"\n",
        "    Given an undirected weight matrix S2 (n x n), compute the bottleneck value\n",
        "    of a maximum spanning tree: min edge weight on the MST (descending weights).\n",
        "    If graph has n<2, returns +inf. If weights are all 0, returns 0.\n",
        "    \"\"\"\n",
        "    n = S2.shape[0]\n",
        "    if n < 2:\n",
        "        return float('inf')\n",
        "\n",
        "    # Prim's algorithm for MAX spanning tree\n",
        "    visited = [False]*n\n",
        "    visited[0] = True\n",
        "    heap = []\n",
        "    for v in range(1, n):\n",
        "        w = S2[0, v]\n",
        "        heapq.heappush(heap, (-w, 0, v))  # max-heap via negative weight\n",
        "\n",
        "    chosen = 0\n",
        "    mins = []  # collect MST edge weights; we'll take min at the end\n",
        "    while heap and chosen < n - 1:\n",
        "        negw, u, v = heapq.heappop(heap)\n",
        "        if visited[v]:\n",
        "            continue\n",
        "        visited[v] = True\n",
        "        w = -negw\n",
        "        mins.append(w)\n",
        "        chosen += 1\n",
        "        for x in range(n):\n",
        "            if not visited[x] and x != v:\n",
        "                heapq.heappush(heap, (-S2[v, x], v, x))\n",
        "\n",
        "    if chosen != n - 1:\n",
        "        # graph had zero/very small weights that don't connect -> bottleneck 0\n",
        "        return 0.0\n",
        "    return min(mins) if mins else 0.0\n",
        "\n",
        "def build_complex_from_scores(S2, S3, tau2, tau3, enforce_closure=True):\n",
        "    \"\"\"\n",
        "    Build closed complex from fixed thresholds tau2 (edges) and tau3 (triangles).\n",
        "    \"\"\"\n",
        "    n = S2.shape[0]\n",
        "    edges = {frozenset((i, j))\n",
        "             for i in range(n) for j in range(i+1, n)\n",
        "             if S2[i, j] >= tau2}\n",
        "\n",
        "    tris  = {t for t, s in S3.items() if s >= tau3}\n",
        "\n",
        "    if enforce_closure:\n",
        "        # add triangle faces\n",
        "        for t in tris:\n",
        "            i, j, k = sorted(t)\n",
        "            edges.add(frozenset((i, j)))\n",
        "            edges.add(frozenset((i, k)))\n",
        "            edges.add(frozenset((j, k)))\n",
        "\n",
        "    return edges, tris\n",
        "\n",
        "\n",
        "# 1. Preselect windows\n",
        "window_meta = preselect_windows(X, WIN_LEN, STRIDE)\n",
        "\n",
        "# 2. Fit SINDy over selected windows (parallel) ‚Üí per-window complexes\n",
        "fit_metas = [m for m in window_meta if not m[\"skip\"]]\n",
        "skipped_count = len(window_meta) - len(fit_metas)\n",
        "print(f\"\\n--- Proceeding to SINDy Fitting ---\")\n",
        "print(f\"Fitting {len(fit_metas)} windows (Skipped {skipped_count} nonlocal windows)\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "results = Parallel(n_jobs=N_JOBS, verbose=5)(\n",
        "    delayed(fit_window_from_meta)(m) for m in fit_metas\n",
        ")\n",
        "print('    done ‚úî')\n",
        "\n",
        "# Keep only successfully fitted windows\n",
        "successful_results = [res for res in results if not res.get(\"skip\", False)]\n",
        "fit_skipped_count = len(results) - len(successful_results)\n",
        "print(f\"Successfully fitted {len(successful_results)} windows (Skipped {fit_skipped_count} during fit)\")\n",
        "# ---- GLOBAL THRESHOLDS ----\n",
        "# Edge threshold ensuring connectivity in *every* window:\n",
        "bottlenecks = []\n",
        "for res in successful_results:\n",
        "    b = mst_bottleneck_value(res['S2'])\n",
        "    bottlenecks.append(b)\n",
        "tau2_global = float(min(bottlenecks)) if bottlenecks else 0.0\n",
        "print(f\"\\nGlobal edge threshold tau2 (connectivity-guaranteeing): {tau2_global:.6g}\")\n",
        "\n",
        "# Triangle threshold: pick once across all windows (quantile or your own criterion)\n",
        "# Example: 97.5th percentile of all triangle scores pooled\n",
        "all_tri_scores = []\n",
        "for res in successful_results:\n",
        "    all_tri_scores.extend(list(res['S3'].values()))\n",
        "if all_tri_scores:\n",
        "    tau3_global = float(np.quantile(all_tri_scores, 0.975))\n",
        "else:\n",
        "    tau3_global = float('inf')  # no triangles possible\n",
        "print(f\"Global triangle threshold tau3: {tau3_global:.6g}\")\n",
        "\n",
        "# ---- REBUILD PER-WINDOW COMPLEXES with (tau2_global, tau3_global) ----\n",
        "for res in successful_results:\n",
        "    E2, T3 = build_complex_from_scores(res['S2'], res['S3'], tau2_global, tau3_global, enforce_closure=True)\n",
        "    res['edges_2'] = E2\n",
        "    res['tris_3']  = T3\n",
        "    res['quads_4'] = set()  # D_MAX=2 here\n",
        "    res['n_edges2'] = len(E2)\n",
        "    res['n_edges3'] = len(T3)\n",
        "    res['n_edges4'] = 0\n",
        "    res['tau2_global'] = tau2_global\n",
        "    res['tau3_global'] = tau3_global\n",
        "\n",
        "# 3. Locality summary across fitted windows\n",
        "all_locality_stats = defaultdict(list)\n",
        "for res in successful_results:\n",
        "    for k in ('p95_raw','p95_kept','n_samples_fit','n_samples_kept','kept_pct'):\n",
        "        all_locality_stats[k].append(res[k])\n",
        "\n",
        "# (Optional) print the fixed global thresholds once:\n",
        "print(f\"\\nFixed thresholds: tau2_global={successful_results[0]['tau2_global']:.6g}, \"\n",
        "      f\"tau3_global={successful_results[0]['tau3_global']:.6g}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOCALITY ANALYSIS SUMMARY (Fitted Windows)\")\n",
        "print(\"=\"*60)\n",
        "for key, vals in all_locality_stats.items():\n",
        "    if vals:\n",
        "        print(f\"  {key}: mean={np.mean(vals):.3f}, std={np.std(vals):.3f}, median={np.median(vals):.3f}\")\n",
        "\n",
        "# 4. Save per-window edge lists + counts (closed complexes)\n",
        "def pretty_edge(e):\n",
        "    return '{' + ','.join(map(str, sorted(e))) + '}'\n",
        "\n",
        "print(f\"\\nüìÅ Saving {len(successful_results)} fitted time slices to '{OUTDIR}/'...\")\n",
        "successful_results.sort(key=lambda r: r[\"t_mid\"])\n",
        "\n",
        "for res in successful_results:\n",
        "    stamp = f'{res[\"t_mid\"]:010.3f}'\n",
        "\n",
        "    with open(f'{OUTDIR}/edges2_{stamp}.txt', 'w') as f2:\n",
        "        f2.write(f'# Taylor center ({CENTER_METHOD}): {np.array(res[\"x0\"])[:3]}...\\n')\n",
        "        f2.write(f'# counts (closed): edges2={res[\"n_edges2\"]}, edges3={res[\"n_edges3\"]}, edges4={res[\"n_edges4\"]}\\n')\n",
        "        f2.write(f'# Locality (PC Norm): P95_raw={res[\"p95_raw\"]:.3f}, P95_kept={res[\"p95_kept\"]:.3f}, Kept_pct={res[\"kept_pct\"]:.1f}%\\n')\n",
        "        f2.write(f'# global thresholds: tau2_global={tau2_global:.6g}, tau3_global={tau3_global:.6g}\\n')\n",
        "        for e in sorted(res['edges_2'], key=lambda s: tuple(sorted(s))):\n",
        "            f2.write(f'{pretty_edge(e)}\\n')\n",
        "\n",
        "    with open(f'{OUTDIR}/edges3_{stamp}.txt', 'w') as f3:\n",
        "        f3.write(f'# Taylor center ({CENTER_METHOD}): {np.array(res[\"x0\"])[:3]}...\\n')\n",
        "        f3.write(f'# counts (closed): edges2={res[\"n_edges2\"]}, edges3={res[\"n_edges3\"]}, edges4={res[\"n_edges4\"]}\\n')\n",
        "        f3.write(f'# Locality (PC Norm): P95_raw={res[\"p95_raw\"]:.3f}, P95_kept={res[\"p95_kept\"]:.3f}, Kept_pct={res[\"kept_pct\"]:.1f}%\\n')\n",
        "        f3.write(f'# global thresholds: tau2_global={tau2_global:.6g}, tau3_global={tau3_global:.6g}\\n')\n",
        "\n",
        "        for e in sorted(res['tris_3'], key=lambda s: tuple(sorted(s))):\n",
        "            f3.write(f'{pretty_edge(e)}\\n')\n",
        "\n",
        "    if res['quads_4']:\n",
        "        with open(f'{OUTDIR}/edges4_{stamp}.txt', 'w') as f4:\n",
        "            f4.write(f'# Taylor center ({CENTER_METHOD}): {np.array(res[\"x0\"])[:3]}...\\n')\n",
        "            f4.write(f'# counts (closed): edges2={res[\"n_edges2\"]}, edges3={res[\"n_edges3\"]}, edges4={res[\"n_edges4\"]}\\n')\n",
        "            f4.write(f'# Locality (PC Norm): P95_raw={res[\"p95_raw\"]:.3f}, P95_kept={res[\"p95_kept\"]:.3f}, Kept_pct={res[\"kept_pct\"]:.1f}%\\n')\n",
        "            f4.write(f'# global thresholds: tau2_global={tau2_global:.6g}, tau3_global={tau3_global:.6g}\\n')\n",
        "            for e in sorted(res['quads_4'], key=lambda s: tuple(sorted(s))):\n",
        "                f4.write(f'{pretty_edge(e)}\\n')\n",
        "\n",
        "# Per-window counts CSV\n",
        "rows = []\n",
        "tau2_global = successful_results[0][\"tau2_global\"] if successful_results else np.nan\n",
        "tau3_global = successful_results[0][\"tau3_global\"] if successful_results else np.nan\n",
        "\n",
        "for r in successful_results:\n",
        "    rows.append({\n",
        "        \"t_mid\": r[\"t_mid\"],\n",
        "        \"n_edges2\": r[\"n_edges2\"],\n",
        "        \"n_edges3\": r[\"n_edges3\"],\n",
        "        \"n_edges4\": r[\"n_edges4\"],\n",
        "        \"tau2_global\": tau2_global,\n",
        "        \"tau3_global\": tau3_global,\n",
        "        \"n_samples_fit\": r[\"n_samples_fit\"],\n",
        "        \"kept_pct\": r[\"kept_pct\"],\n",
        "        \"p95_raw\": r[\"p95_raw\"],\n",
        "        \"p95_kept\": r[\"p95_kept\"],\n",
        "        \"mode\": r[\"locality_mode\"],\n",
        "    })\n",
        "\n",
        "pd.DataFrame(rows).to_csv(f\"{OUTDIR}/hyperedge_counts_per_window.csv\", index=False)\n",
        "print(f\"‚úì Saved per-window counts ‚Üí {OUTDIR}/hyperedge_counts_per_window.csv\")\n",
        "\n",
        "# --- Optional: Hypergraph Structural/Topology Summary (across windows) ---\n",
        "def count_hyperedge_statistics(results_list):\n",
        "    \"\"\"Totals per window (averaged) and unique sets across all windows.\"\"\"\n",
        "    total_counts = {'edges_2': 0, 'edges_3': 0, 'edges_4': 0}\n",
        "    unique_hyperedges = {'edges_2': set(), 'edges_3': set(), 'edges_4': set()}\n",
        "\n",
        "    for res in results_list:\n",
        "        total_counts['edges_2'] += len(res.get('edges_2', set()))\n",
        "        total_counts['edges_3'] += len(res.get('tris_3', set()))\n",
        "        total_counts['edges_4'] += len(res.get('quads_4', set()))\n",
        "\n",
        "        unique_hyperedges['edges_2'].update(res.get('edges_2', set()))\n",
        "        unique_hyperedges['edges_3'].update(res.get('tris_3', set()))\n",
        "        unique_hyperedges['edges_4'].update(res.get('quads_4', set()))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"HYPERGRAPH STRUCTURAL STATISTICS (Across Windows)\")\n",
        "    print(\"=\"*60)\n",
        "    nW = len(results_list)\n",
        "\n",
        "    print(\"\\nAverage per window:\")\n",
        "    for key, count in total_counts.items():\n",
        "        print(f\"  {key}: {count/nW:.1f}\" if nW > 0 else f\"  {key}: 0.0\")\n",
        "\n",
        "    print(\"\\nStructural proportions (unique across all windows):\")\n",
        "    tot_unique = sum(len(s) for s in unique_hyperedges.values())\n",
        "    for key, edges in unique_hyperedges.items():\n",
        "        prop = (len(edges) / tot_unique * 100) if tot_unique > 0 else 0.0\n",
        "        print(f\"  {key}: {prop:.1f}% ({len(edges)} unique total)\")\n",
        "    return total_counts, unique_hyperedges\n",
        "\n",
        "def analyze_hypergraph_topology(results_list):\n",
        "    \"\"\"Unique hyperedges and node participation across all windows.\"\"\"\n",
        "    all_edges = {'2way': [], '3way': [], '4way': []}\n",
        "    for res in results_list:\n",
        "        all_edges['2way'].extend(list(res.get('edges_2', set())))\n",
        "        all_edges['3way'].extend(list(res.get('tris_3', set())))\n",
        "        all_edges['4way'].extend(list(res.get('quads_4', set())))\n",
        "    unique_edges = {k: set(map(frozenset, v)) for k, v in all_edges.items()}\n",
        "\n",
        "    n2, n3, n4 = len(unique_edges['2way']), len(unique_edges['3way']), len(unique_edges['4way'])\n",
        "    tot = n2 + n3 + n4\n",
        "\n",
        "    print(f\"\\nHYPERGRAPH STRUCTURE (Unique edges):\")\n",
        "    print(f\"  2-way: {n2} ({(n2/tot*100 if tot>0 else 0):.1f}%)\")\n",
        "    print(f\"  3-way: {n3} ({(n3/tot*100 if tot>0 else 0):.1f}%)\")\n",
        "    print(f\"  4-way: {n4} ({(n4/tot*100 if tot>0 else 0):.1f}%)\")\n",
        "\n",
        "    node_degrees = {2: defaultdict(int), 3: defaultdict(int), 4: defaultdict(int)}\n",
        "    for edge in unique_edges['2way']:\n",
        "        for node in edge: node_degrees[2][node] += 1\n",
        "    for edge in unique_edges['3way']:\n",
        "        for node in edge: node_degrees[3][node] += 1\n",
        "    for edge in unique_edges['4way']:\n",
        "        for node in edge: node_degrees[4][node] += 1\n",
        "\n",
        "    print(f\"\\nNODE PARTICIPATION (Unique edge counts per node):\")\n",
        "    for order in [2, 3, 4]:\n",
        "        vals = list(node_degrees[order].values())\n",
        "        if vals:\n",
        "            print(f\"  {order}-way: avg degree = {np.mean(vals):.1f}, max = {np.max(vals)}\")\n",
        "        else:\n",
        "            print(f\"  {order}-way: No unique edges found.\")\n",
        "    return unique_edges, node_degrees\n",
        "\n",
        "structural_counts, unique_edges_counts = count_hyperedge_statistics(successful_results)\n",
        "unique_edges_topology, node_degrees   = analyze_hypergraph_topology(successful_results)\n",
        "\n",
        "# Final summary + JSON\n",
        "print(f\"\\nFINAL HYPERGRAPH SUMMARY:\")\n",
        "print(f\"  Nodes: {n} channels\")\n",
        "print(f\"  Unique 2-way edges: {len(unique_edges_topology['2way'])}\")\n",
        "print(f\"  Unique 3-way hyperedges: {len(unique_edges_topology['3way'])}\")\n",
        "print(f\"  Unique 4-way hyperedges: {len(unique_edges_topology['4way'])}\")\n",
        "\n",
        "tot_unique_higher = len(unique_edges_topology['3way']) + len(unique_edges_topology['4way'])\n",
        "tot_unique_edges  = len(unique_edges_topology['2way']) + tot_unique_higher\n",
        "higher_order_fraction = (tot_unique_higher / tot_unique_edges) if tot_unique_edges > 0 else 0.0\n",
        "print(f\"  Higher-order unique edge fraction: {higher_order_fraction*100:.1f}%\")\n",
        "\n",
        "summary = {\n",
        "    'n_windows_total': len(window_meta),\n",
        "    'n_windows_skipped_preselection': skipped_count,\n",
        "    'n_windows_attempted_fit': len(fit_metas),\n",
        "    'n_windows_skipped_during_fit': fit_skipped_count,\n",
        "    'n_windows_successful_fit': len(successful_results),\n",
        "    'window_length_s': WIN_LEN / FS,\n",
        "    'stride_s': STRIDE / FS,\n",
        "    'center_method': CENTER_METHOD,\n",
        "    'degree_max': D_MAX,\n",
        "    'n_channels': n,\n",
        "    'thresholding': {\n",
        "    'strategy': 'fixed_global',\n",
        "    'edge_tau2_global': float(tau2_global),\n",
        "    'triangle_tau3_global': float(tau3_global),\n",
        "    'connectivity_rule': 'tau2_global = min_t bottleneck(MST(S2_t))',\n",
        "    'enforce_closure': ENFORCE_CLOSURE,\n",
        "    'sindy_threshold': THRESH_SINDY\n",
        "    },\n",
        "\n",
        "    'locality_stats_summary_fitted': {\n",
        "        k: {\n",
        "            'mean': float(np.mean(v)),\n",
        "            'std' : float(np.std(v)),\n",
        "            'median': float(np.median(v)),\n",
        "            'min': float(np.min(v)),\n",
        "            'max': float(np.max(v))\n",
        "        } for k, v in all_locality_stats.items() if v\n",
        "    },\n",
        "    'unique_counts': {\n",
        "        'edges2': len(unique_edges_topology['2way']),\n",
        "        'edges3': len(unique_edges_topology['3way']),\n",
        "        'edges4': len(unique_edges_topology['4way']),\n",
        "    },\n",
        "    'higher_order_unique_fraction': float(higher_order_fraction)\n",
        "}\n",
        "\n",
        "with open(f'{OUTDIR}/summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úì Wrote {len(successful_results)} closed complexes to '{OUTDIR}/'\")\n",
        "print(f\"‚úì Saved per-window counts CSV and summary JSON to '{OUTDIR}/'\")\n"
      ],
      "metadata": {
        "id": "10PJGQQCgwuV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}