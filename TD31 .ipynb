{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxAVc8hsvHLH"
      },
      "outputs": [],
      "source": [
        "pip install numpy scipy pandas scikit-learn cvxpy group-lasso pysindy\n",
        "!pip install --upgrade importlib-metadata\n",
        "!pip install --upgrade derivative\n",
        "!pip install --upgrade --force-reinstall pysindy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # Import the drive module from google.colab\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)  # Remount with force_remount=True\n",
        "!find /content/drive/MyDrive/data\n"
      ],
      "metadata": {
        "id": "rBXFvRvPvskn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -----------------------------------------------------------\n",
        "# Enhanced hypergraph reconstruction with:\n",
        "# 1. Taylor expansion centering (dynamic per window)\n",
        "# 2. Contribution ratios (ρ) for each interaction order\n",
        "# 3. Better handling of higher-order interactions\n",
        "# -----------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import savgol_filter\n",
        "import pysindy as ps\n",
        "from joblib import Parallel, delayed\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json # Import json for saving summary\n",
        "\n",
        "# ─────────────── USER PARAMETERS ───────────────────────────\n",
        "CSV_FILE        = '/content/drive/MyDrive/data/eeg/EEG_TD_31_EEGdata.csv'\n",
        "FS              = 256.0          # [Hz]\n",
        "WIN_SG, ORDER   = 13, 3          # SavGol window (odd) & polynomial order\n",
        "D_MAX           = 3              # highest polynomial degree in library (set to 4 for 4-way)\n",
        "THRESH_SINDY    = 0.1            # sparsity threshold λ (STLSQ)\n",
        "HEDGE_THRESH    = 0.05           # cut-off ε when mapping coefs → edges\n",
        "\n",
        "WIN_LEN         = 1024           # samples per sliding window (≈ 4.0 s)\n",
        "STRIDE          = 512            # hop between windows (≈ 2.0 s)\n",
        "MAX_ROWS        = 3000           # cap rows per window during SINDy\n",
        "N_JOBS          = -1             # joblib cores (-1 = all)\n",
        "\n",
        "# New parameters for enhanced version\n",
        "CENTER_METHOD   = 'median'       # 'mean' or 'median' for Taylor expansion center\n",
        "USE_GLOBAL_NORM = True           # Whether to use global normalization\n",
        "COMPUTE_CONTRIB = True           # Whether to compute contribution ratios\n",
        "\n",
        "OUTDIR          = 'dyn_graphs_enhanced'\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "# ───────────────────────────────────────────────────────────\n",
        "\n",
        "# 1 ─── READ (T × n → n × T)\n",
        "raw = pd.read_csv(\n",
        "        CSV_FILE,\n",
        "        header=None,\n",
        "        nrows=None,            # Read ALL data (180,000 samples)\n",
        "        usecols=range(31)      # 31 channels\n",
        "      ).values.astype(np.float64)\n",
        "\n",
        "# Drop constant/all-zero channels\n",
        "tol = 1e-12\n",
        "nz_mask = (np.ptp(raw, axis=0) > tol)\n",
        "if (~nz_mask).any():\n",
        "    dropped = np.where(~nz_mask)[0] + 1\n",
        "    print(f'⚠️  Dropping constant channels (1-based): {dropped.tolist()}')\n",
        "raw = raw[:, nz_mask]\n",
        "raw = raw.T                       # rows = channels, cols = time\n",
        "n, T_raw = raw.shape\n",
        "dt = 1 / FS\n",
        "\n",
        "print(f\"Loaded data: {n} channels × {T_raw} samples ({T_raw/FS:.1f} seconds)\")\n",
        "\n",
        "# 2 ─── SAVITZKY–GOLAY smooth + derivative\n",
        "half = (WIN_SG - 1) // 2\n",
        "# 2. smooth & derivative (both from smoothed trace)\n",
        "X_smooth = savgol_filter(raw, WIN_SG, ORDER, axis=1, mode='interp')\n",
        "X_smooth = X_smooth[:, half:-half]\n",
        "\n",
        "dXdt_raw = savgol_filter(X_smooth, WIN_SG, ORDER,\n",
        "                         deriv=1, delta=dt, axis=1, mode='interp')\n",
        "\n",
        "# 3. normalisation\n",
        "if USE_GLOBAL_NORM:\n",
        "    scale = np.mean(np.abs(X_smooth))\n",
        "    X = X_smooth / scale\n",
        "    Y = dXdt_raw / scale\n",
        "else:\n",
        "    X, Y = X_smooth, dXdt_raw\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Data after preprocessing: {X.shape[0]} channels × {X.shape[1]} samples\")\n",
        "\n",
        "# ─────────── Helper functions ────────────\n",
        "\n",
        "def indices_from_term(term_str):\n",
        "    \"\"\"\n",
        "    Parse a term string to get node indices.\n",
        "    '1'           -> []\n",
        "    'x0'          -> [0]\n",
        "    'x0^2'        -> [0, 0]\n",
        "    'x1^3 x4'     -> [1, 1, 1, 4]\n",
        "    \"\"\"\n",
        "    if term_str == '1':\n",
        "        return []\n",
        "    idxs = []\n",
        "    for tok in term_str.split():\n",
        "        base, *pow_part = tok.split('^')\n",
        "        j = int(base[1:])\n",
        "        power = int(pow_part[0]) if pow_part else 1\n",
        "        idxs.extend([j] * power)\n",
        "    return idxs\n",
        "\n",
        "def compute_contribution_ratios(coefs, feature_names, X_window, max_order=4):\n",
        "    \"\"\"Fixed version that skips repeated indices\"\"\"\n",
        "    n_channels = coefs.shape[0]\n",
        "    contributions = defaultdict(float)\n",
        "\n",
        "    for target in range(n_channels):\n",
        "        for coef, term in zip(coefs[target], feature_names):\n",
        "            if abs(coef) < 1e-10 or term == '1':\n",
        "                continue\n",
        "\n",
        "            indices = indices_from_term(term)\n",
        "            if not indices:\n",
        "                continue\n",
        "\n",
        "            all_nodes = [target] + indices\n",
        "            unique_nodes = set(all_nodes)\n",
        "\n",
        "            # CRITICAL FIX: Skip terms with repeated indices\n",
        "            if len(unique_nodes) <= 1 or len(unique_nodes) != len(all_nodes):\n",
        "                continue  # Skip x0^2, x1^3, etc.\n",
        "\n",
        "            order = len(unique_nodes)\n",
        "            if order > max_order:\n",
        "                continue\n",
        "\n",
        "            # Now compute magnitude (keeping multiplicity for computation)\n",
        "            if len(indices) > 0:\n",
        "                term_values = np.ones(X_window.shape[1])\n",
        "                for idx in indices:\n",
        "                    term_values *= X_window[idx]\n",
        "                avg_magnitude = np.mean(np.abs(term_values))\n",
        "            else:\n",
        "                avg_magnitude = 1.0\n",
        "\n",
        "            contribution = abs(coef) * avg_magnitude\n",
        "            contributions[order] += contribution\n",
        "\n",
        "\n",
        "    # Normalize to get ratios\n",
        "    total = sum(contributions.values())\n",
        "    rho = {}\n",
        "\n",
        "    if total > 1e-10:\n",
        "        for order in range(2, max_order + 1):\n",
        "            rho[order] = contributions[order] / total\n",
        "    else:\n",
        "        for order in range(2, max_order + 1):\n",
        "            rho[order] = 0.0\n",
        "\n",
        "    # Compute higher-order ratio (3-way and above)\n",
        "    rho['higher'] = sum(rho.get(o, 0) for o in range(3, max_order + 1))\n",
        "\n",
        "    return rho\n",
        "\n",
        "def coefs_to_simplices_enhanced(coefs, feature_names, thresh=1e-3):\n",
        "    \"\"\"Properly handle polynomial terms without double-counting\"\"\"\n",
        "    edge_w, tri_w, quad_w = {}, {}, {}\n",
        "\n",
        "    # Parse all features once\n",
        "    parsed = []\n",
        "    for term in feature_names:\n",
        "        if term == '1':\n",
        "            parsed.append(None)\n",
        "        else:\n",
        "            parsed.append(indices_from_term(term))\n",
        "\n",
        "    n = coefs.shape[0]\n",
        "\n",
        "    for target in range(n):\n",
        "        for w, idxs in zip(coefs[target], parsed):\n",
        "            if idxs is None or abs(w) < thresh:\n",
        "                continue\n",
        "\n",
        "            # Critical: Check for genuine multi-way interaction\n",
        "            all_nodes = [target] + idxs\n",
        "            unique_nodes = set(all_nodes)\n",
        "\n",
        "            # Skip if:\n",
        "            # 1. Self-loop (only one unique node)\n",
        "            # 2. Repeated indices (not genuine multi-way)\n",
        "            if len(unique_nodes) <= 1 or len(unique_nodes) != len(all_nodes):\n",
        "                continue\n",
        "\n",
        "            # Now we have a genuine k-way interaction\n",
        "            simplex = frozenset(unique_nodes)\n",
        "            k = len(simplex)\n",
        "\n",
        "            if k == 2:\n",
        "                edge_w[simplex] = max(edge_w.get(simplex, 0.0), abs(w))\n",
        "            elif k == 3:\n",
        "                tri_w[simplex] = max(tri_w.get(simplex, 0.0), abs(w))\n",
        "            elif k == 4:\n",
        "                quad_w[simplex] = max(quad_w.get(simplex, 0.0), abs(w))\n",
        "\n",
        "    return edge_w, tri_w, quad_w, {}\n",
        "GLOBAL_LIBRARY = ps.PolynomialLibrary(degree=D_MAX, include_bias=True)\n",
        "\n",
        "\n",
        "# ─────────── Per-window SINDy with Taylor centering ────────────\n",
        "def fit_window_enhanced(w_start):\n",
        "    \"\"\"\n",
        "    Enhanced window fitting with:\n",
        "    1. Taylor expansion centering\n",
        "    2. Contribution ratio computation\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- Slice window -------------------------------------------------\n",
        "    w_end = w_start + WIN_LEN\n",
        "    Xw, Yw = X[:, w_start:w_end], Y[:, w_start:w_end]\n",
        "\n",
        "    # ---- Center data for Taylor expansion ----------------------------\n",
        "    if CENTER_METHOD == 'mean':\n",
        "        x0 = np.mean(Xw, axis=1, keepdims=True)\n",
        "    elif CENTER_METHOD == 'median':\n",
        "        x0 = np.median(Xw, axis=1, keepdims=True)\n",
        "    else:\n",
        "        x0 = np.zeros((n, 1))  # No centering\n",
        "\n",
        "    # Center the data around x0\n",
        "    Xw_centered = Xw - x0\n",
        "    # Note: derivatives don't need centering\n",
        "\n",
        "    # Optional row-subsample\n",
        "    if Xw_centered.shape[1] > MAX_ROWS:\n",
        "        idx = np.linspace(0, Xw_centered.shape[1]-1, MAX_ROWS, dtype=int)\n",
        "        Xw_use, Yw_use = Xw_centered[:, idx], Yw[:, idx]\n",
        "    else:\n",
        "        Xw_use, Yw_use = Xw_centered, Yw\n",
        "\n",
        "    # ---- SINDy fit ----------------------------------------------------\n",
        "   # library = ps.PolynomialLibrary(degree=D_MAX, include_bias=True)\n",
        "    optimizer = ps.STLSQ(alpha=1e-3, threshold=THRESH_SINDY)\n",
        "    model = ps.SINDy(feature_library=GLOBAL_LIBRARY, optimizer=optimizer)\n",
        "\n",
        "    # Fit on centered data\n",
        "    model.fit(Xw_use.T, t=dt, x_dot=Yw_use.T, quiet=True)\n",
        "\n",
        "    # ---- Extract results ----------------------------------------------\n",
        "    coefs_matrix = model.coefficients()\n",
        "    feature_names = model.get_feature_names()\n",
        "\n",
        "    # Extract simplices\n",
        "    edges_2, tris_3, quads_4, term_contribs = coefs_to_simplices_enhanced(\n",
        "        coefs_matrix, feature_names, thresh=HEDGE_THRESH\n",
        "    )\n",
        "\n",
        "    # ---- Compute contribution ratios ----------------------------------\n",
        "    rho = {}\n",
        "    if COMPUTE_CONTRIB:\n",
        "        rho = compute_contribution_ratios(\n",
        "            coefs_matrix, feature_names, Xw_centered, max_order=D_MAX+1\n",
        "        )\n",
        "\n",
        "    # ---- Return results -----------------------------------------------\n",
        "    t_mid = (w_start + w_end) / 2 * dt\n",
        "\n",
        "    result = {\n",
        "        't_mid': t_mid,\n",
        "        'edges_2': edges_2,\n",
        "        'tris_3': tris_3,\n",
        "        'quads_4': quads_4,\n",
        "        'rho': rho,\n",
        "        'x0': x0,  # Store the centering point\n",
        "        'n_samples': Xw_use.shape[1],\n",
        "        'term_contributions': term_contribs\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# 4 ─── Run over sliding windows (parallel) ─────────\n",
        "starts = range(0, X.shape[1] - WIN_LEN + 1, STRIDE)\n",
        "n_windows = len(starts)\n",
        "print(f'⏳  Fitting {n_windows} windows with Taylor centering ({CENTER_METHOD})...')\n",
        "\n",
        "results = Parallel(n_jobs=N_JOBS, verbose=5)(\n",
        "    delayed(fit_window_enhanced)(ws) for ws in starts\n",
        ")\n",
        "print('    done ✔')\n",
        "\n",
        "# 5 ─── Analyze contribution ratios across windows ─────────\n",
        "all_rho = defaultdict(list)\n",
        "\n",
        "if COMPUTE_CONTRIB:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CONTRIBUTION RATIO ANALYSIS (ρ values)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Collect all rho values\n",
        "    # Initialize at the top:\n",
        "    for res in results:\n",
        "        if res['rho']:\n",
        "            for order, value in res['rho'].items():\n",
        "                all_rho[order].append(value)\n",
        "\n",
        "    # Compute statistics\n",
        "    print(\"\\nAverage contribution by interaction order:\")\n",
        "    for order in [2, 3, 4, 'higher']:\n",
        "        if order in all_rho and all_rho[order]:\n",
        "            values = np.array(all_rho[order])\n",
        "            print(f\"  ρ_{order}: {np.mean(values):.3f} ± {np.std(values):.3f}\")\n",
        "            print(f\"       median: {np.median(values):.3f}, \"\n",
        "                  f\"range: [{np.min(values):.3f}, {np.max(values):.3f}]\")\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    # Plot 1: Distribution of each ρ\n",
        "    ax = axes[0, 0]\n",
        "    data_for_plot = []\n",
        "    labels = []\n",
        "    for order in [2, 3, 4]:\n",
        "        if order in all_rho and all_rho[order]:\n",
        "            data_for_plot.append(all_rho[order])\n",
        "            labels.append(f'ρ_{order}')\n",
        "\n",
        "    if data_for_plot:\n",
        "        bp = ax.boxplot(data_for_plot, labels=labels)\n",
        "        ax.set_ylabel('Contribution Ratio')\n",
        "        ax.set_title('Distribution of Contribution Ratios')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Time evolution of contributions\n",
        "    ax = axes[0, 1]\n",
        "    time_points = [res['t_mid'] for res in results]\n",
        "    for order, color in [(2, 'blue'), (3, 'orange'), (4, 'green')]:\n",
        "        if order in all_rho:\n",
        "            values = [res['rho'].get(order, 0) for res in results]\n",
        "            ax.plot(time_points, values, label=f'ρ_{order}', color=color, alpha=0.7)\n",
        "    ax.set_xlabel('Time (s)')\n",
        "    ax.set_ylabel('Contribution Ratio')\n",
        "    ax.set_title('Temporal Evolution of Contributions')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Higher-order vs Pairwise\n",
        "    ax = axes[1, 0]\n",
        "    if 'higher' in all_rho:\n",
        "        higher_values = all_rho['higher']\n",
        "        ax.hist(higher_values, bins=30, edgecolor='black', alpha=0.7)\n",
        "        ax.axvline(np.mean(higher_values), color='red', linestyle='--',\n",
        "                  label=f'Mean = {np.mean(higher_values):.3f}')\n",
        "        ax.set_xlabel('Higher-Order Contribution (ρ₃ + ρ₄)')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.set_title('Distribution of Higher-Order Contributions')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Scatter plot of ρ₂ vs ρ_higher\n",
        "    ax = axes[1, 1]\n",
        "    if 2 in all_rho and 'higher' in all_rho:\n",
        "        ax.scatter(all_rho[2], all_rho['higher'], alpha=0.5)\n",
        "        ax.set_xlabel('ρ₂ (Pairwise)')\n",
        "        ax.set_ylabel('ρ_higher (3-way+)')\n",
        "        ax.set_title('Pairwise vs Higher-Order Contributions')\n",
        "        ax.plot([0, 1], [1, 0], 'k--', alpha=0.3)  # Reference line\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTDIR}/contribution_ratios.png', dpi=150)\n",
        "    print(f\"\\n✓ Saved contribution ratio analysis to {OUTDIR}/contribution_ratios.png\")\n",
        "\n",
        "    # Statistical test: Are higher-order interactions significant?\n",
        "    if 'higher' in all_rho:\n",
        "        higher_vals = np.array(all_rho['higher'])\n",
        "        prop_significant = np.mean(higher_vals > 0.3)  # Arbitrary threshold\n",
        "        print(f\"\\n{prop_significant*100:.1f}% of windows have >30% higher-order contribution\")\n",
        "\n",
        "        # Compare to paper's finding (>60% from higher-order)\n",
        "        prop_dominant = np.mean(higher_vals > 0.6)\n",
        "        print(f\"{prop_dominant*100:.1f}% of windows have >60% higher-order contribution\")\n",
        "\n",
        "# 6 ─── Save enhanced edge lists with metadata ───────────\n",
        "def pretty_edge(e):\n",
        "    return '{' + ','.join(map(str, sorted(e))) + '}'\n",
        "\n",
        "print(f\"\\n📁 Saving {len(results)} time slices to '{OUTDIR}/'...\")\n",
        "\n",
        "# Also save summary statistics\n",
        "summary = {\n",
        "    'n_windows': len(results),\n",
        "    'window_length_s': WIN_LEN / FS,\n",
        "    'stride_s': STRIDE / FS,\n",
        "    'center_method': CENTER_METHOD,\n",
        "    'degree_max': D_MAX,\n",
        "    'n_channels': n,\n",
        "    'contribution_stats': {}\n",
        "}\n",
        "\n",
        "for order in [2, 3, 4, 'higher']:\n",
        "    if order in all_rho and all_rho[order]:\n",
        "        summary['contribution_stats'][f'rho_{order}'] = {\n",
        "            'mean': float(np.mean(all_rho[order])),\n",
        "            'std': float(np.std(all_rho[order])),\n",
        "            'median': float(np.median(all_rho[order])),\n",
        "            'min': float(np.min(all_rho[order])),\n",
        "            'max': float(np.max(all_rho[order]))\n",
        "        }\n",
        "\n",
        "# Save individual windows\n",
        "for res in results:\n",
        "    stamp = f'{res[\"t_mid\"]:010.3f}'\n",
        "\n",
        "    # Save edges\n",
        "    with open(f'{OUTDIR}/edges2_{stamp}.txt', 'w') as f2:\n",
        "        f2.write(f'# Taylor center ({CENTER_METHOD}): {res[\"x0\"].flatten()[:3]}...\\n')\n",
        "        f2.write(f'# ρ₂={res[\"rho\"].get(2, 0):.3f}, ρ₃={res[\"rho\"].get(3, 0):.3f}\\n')\n",
        "        for e, w in sorted(res['edges_2'].items(), key=lambda x: -x[1]):\n",
        "            f2.write(f'{pretty_edge(e)} {w:.4f}\\n')\n",
        "\n",
        "    with open(f'{OUTDIR}/edges3_{stamp}.txt', 'w') as f3:\n",
        "        for e, w in sorted(res['tris_3'].items(), key=lambda x: -x[1]):\n",
        "            f3.write(f'{pretty_edge(e)} {w:.4f}\\n')\n",
        "\n",
        "    if res['quads_4']:\n",
        "        with open(f'{OUTDIR}/edges4_{stamp}.txt', 'w') as f4:\n",
        "            for e, w in sorted(res['quads_4'].items(), key=lambda x: -x[1]):\n",
        "                f4.write(f'{pretty_edge(e)} {w:.4f}\\n')\n",
        "\n",
        "# Save summary\n",
        "with open(f'{OUTDIR}/summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"✓ Wrote {len(results)} dynamic slices to '{OUTDIR}/'\")\n",
        "print(f\"✓ Saved summary statistics to '{OUTDIR}/summary.json'\")\n",
        "\n",
        "# --- Run the analysis functions ---\n",
        "\n",
        "def count_hyperedge_statistics(results):\n",
        "    \"\"\"\n",
        "    Count the actual hypergraph structure, not dynamical contributions\n",
        "    \"\"\"\n",
        "    total_counts = {\n",
        "        'edges_2': 0,  # 1-simplices (edges)\n",
        "        'edges_3': 0,  # 2-simplices (triangles)\n",
        "        'edges_4': 0   # 3-simplices (tetrahedra)\n",
        "    }\n",
        "\n",
        "    unique_hyperedges = {\n",
        "        'edges_2': set(),\n",
        "        'edges_3': set(),\n",
        "        'edges_4': set()\n",
        "    }\n",
        "\n",
        "    for res in results:\n",
        "        # Count per window\n",
        "        total_counts['edges_2'] += len(res['edges_2'])\n",
        "        total_counts['edges_3'] += len(res['tris_3'])\n",
        "        total_counts['edges_4'] += len(res['quads_4'])\n",
        "\n",
        "        # Track unique across all windows\n",
        "        for edge in res['edges_2']:\n",
        "            unique_hyperedges['edges_2'].add(frozenset(edge))\n",
        "        for tri in res['tris_3']:\n",
        "            unique_hyperedges['edges_3'].add(frozenset(tri))\n",
        "        for quad in res['quads_4']:\n",
        "            unique_hyperedges['edges_4'].add(frozenset(quad))\n",
        "\n",
        "    # Compute structural statistics\n",
        "    total_edges_count = sum(total_counts.values())\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"HYPERGRAPH STRUCTURAL STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nAverage per window:\")\n",
        "    n_windows = len(results)\n",
        "    for key, count in total_counts.items():\n",
        "         print(f\"  {key}: {count/n_windows:.1f}\")\n",
        "\n",
        "\n",
        "    print(\"\\nStructural proportions (total unique edges):\")\n",
        "    total_unique_edges_count = sum(len(s) for s in unique_hyperedges.values())\n",
        "    for key, edges in unique_hyperedges.items():\n",
        "        proportion = len(edges) / total_unique_edges_count if total_unique_edges_count > 0 else 0\n",
        "        print(f\"  {key}: {proportion*100:.1f}% ({len(edges)} unique total)\")\n",
        "\n",
        "\n",
        "    return total_counts, unique_hyperedges\n",
        "\n",
        "def analyze_hypergraph_topology(results):\n",
        "    \"\"\"\n",
        "    Analyze the actual hypergraph structure\n",
        "    \"\"\"\n",
        "    # Collect all hyperedges\n",
        "    all_edges = {'2way': [], '3way': [], '4way': []}\n",
        "\n",
        "    for res in results:\n",
        "        all_edges['2way'].extend(list(res['edges_2'].keys()))\n",
        "        all_edges['3way'].extend(list(res['tris_3'].keys()))\n",
        "        all_edges['4way'].extend(list(res['quads_4'].keys()))\n",
        "\n",
        "    # Convert to sets to get unique\n",
        "    unique_edges = {\n",
        "        k: set(map(frozenset, v)) for k, v in all_edges.items()\n",
        "    }\n",
        "\n",
        "    # Compute statistics\n",
        "    n2 = len(unique_edges['2way'])\n",
        "    n3 = len(unique_edges['3way'])\n",
        "    n4 = len(unique_edges['4way'])\n",
        "    total = n2 + n3 + n4\n",
        "\n",
        "    print(f\"\\nHYPERGRAPH STRUCTURE (Unique edges):\")\n",
        "    print(f\"  2-way edges: {n2} ({n2/total*100:.1f}%)\" if total > 0 else f\"  2-way edges: {n2} (0.0%)\")\n",
        "    print(f\"  3-way edges: {n3} ({n3/total*100:.1f}%)\" if total > 0 else f\"  3-way edges: {n3} (0.0%)\")\n",
        "    print(f\"  4-way edges: {n4} ({n4/total*100:.1f}%)\" if total > 0 else f\"  4-way edges: {n4} (0.0%)\")\n",
        "\n",
        "\n",
        "    # Degree distribution (based on unique edges)\n",
        "    node_degrees = {2: defaultdict(int), 3: defaultdict(int), 4: defaultdict(int)}\n",
        "\n",
        "    for edge in unique_edges['2way']:\n",
        "        for node in edge:\n",
        "            node_degrees[2][node] += 1\n",
        "\n",
        "    for edge in unique_edges['3way']:\n",
        "        for node in edge:\n",
        "            node_degrees[3][node] += 1\n",
        "\n",
        "    for edge in unique_edges['4way']:\n",
        "        for node in edge:\n",
        "            node_degrees[4][node] += 1\n",
        "\n",
        "    print(f\"\\nNODE PARTICIPATION (Unique edge counts per node):\")\n",
        "    for order in [2, 3, 4]:\n",
        "        if node_degrees[order]:\n",
        "            avg_degree = np.mean(list(node_degrees[order].values()))\n",
        "            max_degree = max(node_degrees[order].values())\n",
        "            print(f\"  {order}-way: avg degree = {avg_degree:.1f}, max = {max_degree}\")\n",
        "        else:\n",
        "             print(f\"  {order}-way: No unique edges found.\")\n",
        "\n",
        "    return unique_edges, node_degrees\n",
        "\n",
        "# --- Run the analysis functions ---\n",
        "\n",
        "structural_counts, unique_edges_counts = count_hyperedge_statistics(results)\n",
        "unique_edges_topology, node_degrees = analyze_hypergraph_topology(results)\n",
        "\n",
        "# This tells you the actual hypergraph structure based on unique edges\n",
        "print(f\"\\nFINAL HYPERGRAPH SUMMARY:\")\n",
        "print(f\"  Nodes: {n} channels\")\n",
        "print(f\"  Unique 2-way edges: {len(unique_edges_topology['2way'])}\")\n",
        "print(f\"  Unique 3-way hyperedges: {len(unique_edges_topology['3way'])}\")\n",
        "print(f\"  Unique 4-way hyperedges: {len(unique_edges_topology['4way'])}\")\n",
        "\n",
        "total_unique_higher_order = len(unique_edges_topology['3way']) + len(unique_edges_topology['4way'])\n",
        "total_unique_edges = len(unique_edges_topology['2way']) + total_unique_higher_order\n",
        "\n",
        "# This is your key metric!\n",
        "higher_order_fraction = total_unique_higher_order / total_unique_edges if total_unique_edges > 0 else 0\n",
        "print(f\"  Higher-order unique edge fraction: {higher_order_fraction*100:.1f}%\")"
      ],
      "metadata": {
        "id": "-feP76sNvh4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}